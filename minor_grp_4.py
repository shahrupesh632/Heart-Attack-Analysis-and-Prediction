# -*- coding: utf-8 -*-
"""Minor_grp_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ohu2PAgtljxRhMuFC3Q1_ijqzrfN33hG
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
files.upload()

# Import Data
import pandas as pd

df = pd.read_csv("heart.csv")
df

df.isnull().sum()

"""Checking for null values in the data.
Since, the sum of null values for all the features is 0, we look for unique values.
"""

df.nunique()

"""After counting the unique values of different features, we can observe that many of them appear to be categorical data, such as sex and chest pain type. Therefore, we can divide the features into continuous and categorical features to gain insights on the data using different types of graph visualizations for each feature type."""

categorical_features=['sex','cp','fbs','restecg','exng','slp','caa','thall']
continuous_features=['age','trtbps','chol','thalachh','oldpeak']
target=['output']

print("Categorical features are: ",categorical_features)
print("Categorical features are: ",continuous_features)
print("Categorical features are: ",target)

"""Distinguishing the dataset into two seperate feature set, categorical and continuous """

df[continuous_features].describe()

df[df.duplicated()]

df.drop_duplicates(keep='first',inplace=True)

df.shape

df.describe()

"""# Exploratory Data Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

for i in categorical_features:
  sns.countplot(data=df,x=i)
  plt.show()
  print("\n\n")

# # len(df[continuous_features])
# len(df)
# print(type(target))
# t=target[0]
# # t=df[target]
# type(t)

"""Countplot"""

for xf in continuous_features:
  sns.scatterplot(data=df,x=xf,y=np.zeros(len(df[xf])),hue=df[target[0]])
  # plt.legend()
  plt.show()
  print("\n\n")

for i in continuous_features:
  sns.boxenplot(y=df[i],width=0.6)
  plt.show()
  print("\n\n")

from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, roc_curve
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

df1 = df

# define the columns to be encoded and scaled
cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']
con_cols = ["age","trtbps","chol","thalachh","oldpeak"]

# encoding the categorical columns
df1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)

# defining the features and target
X = df1.drop(['output'],axis=1)
y = df1[['output']]

# instantiating the scaler
scaler = RobustScaler()

# scaling the continuous featuree
X[con_cols] = scaler.fit_transform(X[con_cols])
print("The first 5 rows of X are")
X.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
print("The shape of X_train is      ", X_train.shape)
print("The shape of X_test is       ",X_test.shape)
print("The shape of y_train is      ",y_train.shape)
print("The shape of y_test is       ",y_test.shape)

"""# Logistic Regression"""

logreg = LogisticRegression()

# fitting the object
logreg.fit(X_train, np.ravel(y_train))

# calculating the probabilities
y_pred_proba = logreg.predict_proba(X_test)

# finding the predicted valued
y_pred = np.argmax(y_pred_proba,axis=1)

# printing the test accuracy
print("The test accuracy score of Logistic Regression is ", accuracy_score(y_test, y_pred))
print("The test recall score of Logistic Regression is ", recall_score(y_test, y_pred))
print("The test precision score of Logistic Regression is ", precision_score(y_test, y_pred))
print("The test F1 score of Logistic Regression is ", f1_score(y_test, y_pred))

y_pred_prob = logreg.predict_proba(X_test)[:,1]

# instantiating the roc_cruve
fpr,tpr,threshols=roc_curve(y_test,y_pred_prob)

# plotting the curve
plt.plot([0,1],[0,1],"k--",'r+')
plt.plot(fpr,tpr,label='Logistic Regression')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Logistric Regression ROC Curve")
plt.show()

"""# Decision Trees"""

dt = DecisionTreeClassifier(random_state = 42)

# fitting the model
dt.fit(X_train, y_train)

# calculating the predictions
y_pred = dt.predict(X_test)

# printing the test accuracy
print("The test accuracy score of Decision Trees is ", accuracy_score(y_test, y_pred))
print("The test recall score of Decision Trees is ", recall_score(y_test, y_pred))
print("The test precision score of Decision Trees is ", precision_score(y_test, y_pred))
print("The test F1 score of Decision Trees is ", f1_score(y_test, y_pred))

"""# Random Forest"""

rf = RandomForestClassifier()

# fitting the model
rf.fit(X_train, np.ravel(y_train))

# calculating the predictions
y_pred = dt.predict(X_test)

# printing the test accuracy

print("The test accuracy score of Random Forest is ", accuracy_score(y_test, y_pred))
print("The test recall score of Random Forest is ", recall_score(y_test, y_pred))
print("The test precision score of Random Forest is ", precision_score(y_test, y_pred))
print("The test F1 score of Random Forest is ", f1_score(y_test, y_pred))

"""# K-Means"""

model = KNeighborsClassifier(n_neighbors = 3)  
model.fit(X_train, np.ravel(y_train))
predicted = model.predict(X_test)
  

# print(confusion_matrix(y_test, predicted))

print("The test accuracy score of KNN is ", accuracy_score(y_test, predicted))
print("The test recall score of KNN is ", recall_score(y_test, predicted))
print("The test precision score of KNN is ", precision_score(y_test, predicted))
print("The test F1 score of KNN is ", f1_score(y_test, predicted))

"""# Gaussian Naive Bayes"""

model = GaussianNB()
model.fit(X_train, np.ravel(y_train))
  
predicted = model.predict(X_test)
  

print("The test accuracy score of Gaussian Naive Bayes is ", accuracy_score(y_test, predicted))
print("The test recall score of Gaussian Naive Bayes is ", recall_score(y_test, predicted))
print("The test precision score of Gaussian Naive Bayes is ", precision_score(y_test, predicted))
print("The test F1 score of Gaussian Naive Bayes is ", f1_score(y_test, predicted))

"""# Support Vector Machine (SVM)"""

# clf = SVC(kernel='linear', C=1, random_state=42).fit(X_train,np.ravel(y_train))

# # predicting the values
# y_pred = clf.predict(X_test)

# printing the test accuracy
model = SVC()
model.fit(X_train, np.ravel(y_train))
  
y_pred = model.predict(X_test)
print("The test accuracy score of SVM is ", accuracy_score(y_test, y_pred))
print("The test recall score of SVM is ", recall_score(y_test, y_pred))
print("The test precision score of SVM is ", precision_score(y_test, y_pred))
print("The test F1 score of SVM is ", f1_score(y_test, y_pred))

